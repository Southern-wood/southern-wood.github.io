<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>如何在共享的 Linux 服务器上合理使用资源 :: Wood&#39;s Blog</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="总结一些在 Linux 服务器上进行资源管理、并行化脚本设置的经验，包括如何监控和优化 CPU、GPU 的使用" />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="https://Southern-wood.github.io/posts/2025-01-23_how-to-use-resources-on-a-shared-linux-server/" />





  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/dark.min.b38cb27d4bd6b8dbdbba0e1c2306105e4555cd2bb5b9f8d53daf8ea4a14942ea.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/light.min.5377cedef12ed001ed6df527ad1b2b2f2afc2a3c032e19165fb9f9375a6ef866.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css">

  
  <link rel="stylesheet" href="https://Southern-wood.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">








  


  

<script>window.THEME_CSS = "{\"dark\":\"https://Southern-wood.github.io/css/dark.min.b38cb27d4bd6b8dbdbba0e1c2306105e4555cd2bb5b9f8d53daf8ea4a14942ea.css\",\"light\":\"https://Southern-wood.github.io/css/light.min.5377cedef12ed001ed6df527ad1b2b2f2afc2a3c032e19165fb9f9375a6ef866.css\"}";</script>



  
  <script src="https://Southern-wood.github.io/js/head.min.c5c6e014b2e6e864a6a83f0f472a4b87ffc714a99a170fb8752072daaf7603c3.js"></script>








<link rel="shortcut icon" href="https://Southern-wood.github.io/favicon.png">
<link rel="apple-touch-icon" href="https://Southern-wood.github.io/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="如何在共享的 Linux 服务器上合理使用资源">
<meta property="og:description" content="总结一些在 Linux 服务器上进行资源管理、并行化脚本设置的经验，包括如何监控和优化 CPU、GPU 的使用" />
<meta property="og:url" content="https://Southern-wood.github.io/posts/2025-01-23_how-to-use-resources-on-a-shared-linux-server/" />
<meta property="og:site_name" content="Wood&#39;s Blog" />

  <meta property="og:image" content="https://Southern-wood.github.io/og-image.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">

  <meta property="article:section" content="tech blog" />

  <meta property="article:section" content="academic" />


  <meta property="article:published_time" content="2025-01-23 00:00:00 &#43;0800 &#43;0800" />










<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" crossorigin="anonymous">

<style>
 
.katex-display { margin: 1rem 0; }
.katex { font-size: 0.95em; }
</style>


</head>
<body>


<div class="container center">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/posts">
  <div class="logo">
    Wood&#39;s Blog
  </div>
</a>

    </div>
    
    <div class="header__theme">
      <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme"></button>
    </div>
    
    <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/">Biography</a></li>
        
      
        
          <li><a href="/posts">Blog</a></li>
        
      
        
          <li><a href="/handouts">Handouts</a></li>
        
      
        
          <li><a href="/tags">Tags</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
  <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/" >Biography</a></li>
        
      
        
          <li><a href="/posts" >Blog</a></li>
        
      
        
          <li><a href="/handouts" >Handouts</a></li>
        
      
        
          <li><a href="/tags" >Tags</a></li>
        
      
      
    
  </ul>
</nav>

  




<script src="https://Southern-wood.github.io/js/header.min.7ff7a2b9c0b78f90c1e544e95c9970508ed2c376e518daf3c9e0064860d5b447.js"></script>

</header>

  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="https://Southern-wood.github.io/posts/2025-01-23_how-to-use-resources-on-a-shared-linux-server/">如何在共享的 Linux 服务器上合理使用资源</a>
  </h1>
  <div class="post-meta"><time class="post-date">2025-01-23&nbsp;[Updated 2025-01-23]</time><span class="post-reading-time">6 min read (2717 words)</span></div>

  
    <span class="post-tags">
      
      #<a href="https://Southern-wood.github.io/tags/operating-systems/">Operating Systems</a>&nbsp;
      
      #<a href="https://Southern-wood.github.io/tags/ubuntu/">Ubuntu</a>&nbsp;
      
      #<a href="https://Southern-wood.github.io/tags/programming/">Programming</a>&nbsp;
      
      #<a href="https://Southern-wood.github.io/tags/shell-scripting/">Shell Scripting</a>&nbsp;
      
    </span>
  
  


  

  <div class="post-content"><div>
        <p>我目前在外校的网络安全实验室线上实习，实验室有一台共享的 Linux 服务器，我至今为止的所有工作几乎都是在这台服务器上完成的。</p>
<p>尽管有一定的 Linux 使用经验，但是我还是在整个实习过程中遇到了比较多的问题。究其原因，使用一台属于自己的 Linux 本地机器，和作为一个普通用户访问共享的 Linux 服务器，这之间还是有较大的区别的。</p>
<p>这篇文章里，我想分享下我使用服务器中用到的一些工具，遇到的问题以及解决方案。</p>
<h2 id="ssh-连接工具">SSH 连接工具<a href="#ssh-连接工具" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>我找不出任何不推荐 VS Code Remote-SSH 插件的理由。</p>
<p>只需要简单的配置，你就可以使用本地的 VS Code 终端编辑远程服务器上的文件，同时，所有的插件和配置都能方便地同步到远程服务器上，体验上和本地写代码并没有区别。</p>
<p>最重要的是，Github Copilot 也能在远程服务器上方便地访问。使用过 AI Copilot 后，我很难回到没有 AI 插件的环境了，由俭入奢易，由奢入俭难啊。</p>
<h2 id="并行化工具">并行化工具<a href="#并行化工具" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>在服务器上，我经常需要同时运行多个任务。特别常见的场景是，同一段代码需要在不同的数据集或者不同的参数设置下进行评估。</p>
<p>这种情况如果在本地的机器，我可能会用 GNU Parallel；但是服务器上并没有安装这个工具。作为一个替代方案，我常用的方式是手动写一个 shell 脚本来管理任务。</p>
<p>例如，main.py 这个程序需要在 10 个数据集上运行，我写的 shell 脚本可能会像这样：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#!/bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in <span class="o">{</span>1..10<span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="k">do</span>
</span></span><span class="line"><span class="cl">    nohup python main.py --dataset dataset_<span class="nv">$i</span> 2&gt;<span class="p">&amp;</span><span class="m">1</span> &gt; output_<span class="nv">$i</span>.log <span class="p">&amp;</span>
</span></span><span class="line"><span class="cl"><span class="k">done</span>
</span></span></code></pre></div><p>这里的 nohup 是用来创建后台进程，如果不使用 nohup 直接运行 python main.py，在 SSH 连接中断后程序会停止运行；&amp; 是用来将任务放到后台运行，和当前的终端分离。</p>
<p>当然，这是只是最粗糙的并行脚本，它的问题有很多，最容易想到的有两点：</p>
<ul>
<li>一些任务可能会占用太多的 CPU/GPU 资源，导致服务器上的其他任务无法正常运行。</li>
<li>所有任务的输出都被 nohup 默认重定向到 nohup.out 文件，这样会导致输出混乱，不方便查看。</li>
</ul>
<p>所以，我实际上会在脚本里手动控制并行任务的数量，并将输出重定向到不同的文件中。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#!/bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="nv">max_jobs</span><span class="o">=</span><span class="m">5</span>
</span></span><span class="line"><span class="cl"><span class="nv">current_jobs</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in <span class="o">{</span>1..10<span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="k">do</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> <span class="nv">$current_jobs</span> -ge <span class="nv">$max_jobs</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nb">wait</span> -n
</span></span><span class="line"><span class="cl">        <span class="nv">current_jobs</span><span class="o">=</span><span class="k">$((</span>current_jobs-1<span class="k">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">    nohup python main.py --dataset <span class="nv">$i</span> 2&gt;<span class="p">&amp;</span><span class="m">1</span> &gt; output_<span class="nv">$i</span>.log <span class="p">&amp;</span>
</span></span><span class="line"><span class="cl">    <span class="nv">current_jobs</span><span class="o">=</span><span class="k">$((</span>current_jobs+1<span class="k">))</span>
</span></span><span class="line"><span class="cl"><span class="k">done</span>
</span></span></code></pre></div><h2 id="cpu-资源控制">CPU 资源控制<a href="#cpu-资源控制" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>虽然上面展示的仍然是一个比较粗糙和简单的脚本，但是已经能满足我的需求了，我在几个月内都在使用这样的脚本来完成任务。</p>
<p>直到最近，共用服务器的其他老师反馈说我的任务占用了太多的 CPU 资源，导致他们的实验无法正常进行了，并提醒我要限制 numpy 的线程数。我打开 htop 一看，才发现所有的 CPU 都在满载状态，load average 高达 700~800（服务器有 112 个逻辑 CPU），这时我才意识到问题的严重性。</p>
<p>紧急 pkill 了所有任务来释放所有资源之后，我按照老师提示的方法设置了 numpy 的线程数，问题才得以解决。</p>
<p>事后复盘发现，我当时在实验的代码是一段 python 代码，其中的 numpy 模块运行时默认会尝试用所有 CPU 核心进行多线程优化，这是因为 numpy 调用的底层是一些高性能计算库，这些计算库会尽可能利用所有的 CPU. 为了限制这些高性能库榨干服务器的计算资源，需要在调用 numpy 之前先设置环境变量，使这些库认为只有少量的 CPU 可以使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;OMP_NUM_THREADS&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;4&#34;</span>  <span class="c1"># OpenMP 线程数</span>
</span></span><span class="line"><span class="cl"><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;OPENBLAS_NUM_THREADS&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;4&#34;</span>  <span class="c1"># OpenBLAS 线程数</span>
</span></span><span class="line"><span class="cl"><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;MKL_NUM_THREADS&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;4&#34;</span>  <span class="c1"># MKL 线程数</span>
</span></span><span class="line"><span class="cl"><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;NUMEXPR_NUM_THREADS&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;4&#34;</span>  <span class="c1"># NumExpr 线程数</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span></code></pre></div><p>在设置这些变量后重新运行一段时间后，服务器上出现了其他问题：内存占用过高，可用内存仅剩 10GB/400GB。经过 htop 查看和分析后，导致这些问题的仍然是我的并行任务&hellip;</p>
<p>对于内存的限制还是比较容易的，我在 shell 脚本的最上面加了一行 <code>ulimit -v</code>，限制整个脚本的内存使用量。</p>
<h2 id="硬盘空间管理">硬盘空间管理<a href="#硬盘空间管理" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>服务器上的硬盘空间是有限的，但是 home 文件夹的膨胀是没有止境的。</p>
<p>好在实验室有额外加装的硬盘，挂载在 <code>/data</code> 目录下。
最开始，我只是把不太常用的数据集放在了 <code>/data</code> 目录下。但后来随着使用时间的增加，home 文件夹下各类文件也越来越多。按照实验室建议，我开始把一些比较常用，但是占用了较大空间的文件夹都挪到了 <code>/data</code> 目录下，然后用 <code>ln -s</code> 重新软链接回 home 文件夹。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">mv ~/folder /data/my_user_name/folder
</span></span><span class="line"><span class="cl">ln -s /data/my_user_name/folder ~/folder
</span></span></code></pre></div><h2 id="gpu-资源管理">GPU 资源管理<a href="#gpu-资源管理" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>深度学习的任务十分依赖 GPU 加速。服务器上有若干 RTX 4090，但是由于服务器上有其他用户的实验，我们不能随意使用所有的 GPU 资源。</p>
<p>这一部分实际上分为两个话题，一个是我如何知道自己占用了多少显存，一个是如何合理地分配任务到不同的 GPU 上。</p>
<h3 id="监控-gpu-资源">监控 GPU 资源<a href="#监控-gpu-资源" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>查看 GPU 的状态可以使用 nividia 提供的 nvidia-smi 工具，这个工具可以查看 GPU 的使用情况，包括 GPU 的使用率、温度、显存使用情况等等。</p>
<p>这个工具当然很好用，不过，我需要看到这些显存具体是被哪个用户占用的。更具体地来说，我想要看到我自己占用了多少显存，同时，其他用户中是否有人也正在重度使用 GPU，我是否需要管理自己的任务，降低对服务器的影响。</p>
<p>因此我在 AI 工具辅助下编写了一个简单的脚本，这个脚本可以查看当前所有用户的显存使用情况，各个 GPU 剩余的显存，以及当前用户的内存使用情况。</p>
<p>为了方便高亮查看，我把整个脚本的核心代码贴在下面，去掉了 watch -n 1 的前缀。实际使用的脚本中，我使用 watch -n 1 来每秒刷新一次脚本显示，实现实时监控的效果。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="cp">#!/bin/sh
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl">nvidia-smi --query-compute-apps<span class="o">=</span>pid,used_memory --format<span class="o">=</span>csv,noheader,nounits <span class="p">|</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span><span class="k">while</span> <span class="nv">IFS</span><span class="o">=</span><span class="s1">&#39;,&#39;</span> <span class="nb">read</span> -r pid memory<span class="p">;</span> <span class="k">do</span> 
</span></span><span class="line"><span class="cl">  <span class="nv">user</span><span class="o">=</span><span class="k">$(</span>ps -o <span class="nv">user</span><span class="o">=</span> -p <span class="nv">$pid</span><span class="k">)</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">  <span class="nb">echo</span> <span class="s2">&#34;</span><span class="nv">$user</span><span class="s2">,</span><span class="nv">$memory</span><span class="s2">&#34;</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl"><span class="k">done</span> <span class="p">|</span> awk -F<span class="s2">&#34;,&#34;</span> <span class="s1">&#39;&#34;&#39;</span><span class="s2">&#34;&#39;{mem[</span><span class="nv">$1</span><span class="s2">]+=</span><span class="nv">$2</span><span class="s2">} END {for (u in mem) print u, mem[u] &#34;</span> MiB<span class="s2">&#34;}&#39;&#34;</span><span class="s1">&#39;&#34;&#39;</span> <span class="p">|</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        awk -v <span class="nv">current_user</span><span class="o">=</span><span class="s2">&#34;&#39;</span><span class="k">$(</span>whoami<span class="k">)</span><span class="s2">&#39;&#34;</span> <span class="s1">&#39;&#34;&#39;</span><span class="s2">&#34;&#39;{if (</span><span class="nv">$1</span><span class="s2"> == current_user) printf &#34;</span>%-10s %-10s %-10.2f GB %-10.2f Cards &lt;<span class="o">==</span><span class="se">\n</span><span class="s2">&#34;, </span><span class="nv">$1</span><span class="s2">, </span><span class="nv">$2</span><span class="s2">, </span><span class="nv">$2</span><span class="s2">/1024, </span><span class="nv">$2</span><span class="s2">/24576; else printf &#34;</span>%-10s %-10s %-10.2f GB %-10.2f Cards<span class="se">\n</span><span class="s2">&#34;, </span><span class="nv">$1</span><span class="s2">, </span><span class="nv">$2</span><span class="s2">, </span><span class="nv">$2</span><span class="s2">/1024, </span><span class="nv">$2</span><span class="s2">/24576}&#39;&#34;</span><span class="s1">&#39;&#34;&#39;</span> <span class="p">|</span> sort -k3 -nr <span class="p">|</span> column -t
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;==============================================&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Memory usage by me:&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">CURRENT_USER</span><span class="o">=</span><span class="k">$(</span>whoami<span class="k">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">TOTAL_MEMORY</span><span class="o">=</span><span class="k">$(</span>ps -u <span class="s2">&#34;</span><span class="nv">$CURRENT_USER</span><span class="s2">&#34;</span> -o <span class="nv">rss</span><span class="o">=</span> <span class="p">|</span> awk <span class="s1">&#39;&#34;&#39;</span><span class="s2">&#34;&#39;{sum+=</span><span class="nv">$1</span><span class="s2">} END {print sum}&#39;&#34;</span><span class="s1">&#39;&#34;&#39;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">TOTAL_MEMORY_MB</span><span class="o">=</span><span class="k">$((</span>TOTAL_MEMORY <span class="o">/</span> <span class="m">1024</span><span class="k">))</span>
</span></span><span class="line"><span class="cl"><span class="nv">TOTAL_MEMORY_GB</span><span class="o">=</span><span class="k">$((</span>TOTAL_MEMORY_MB <span class="o">/</span> <span class="m">1024</span><span class="k">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Total memory usage: </span><span class="nv">$TOTAL_MEMORY_MB</span><span class="s2"> MB (</span><span class="nv">$TOTAL_MEMORY_GB</span><span class="s2"> GB)&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;==============================================&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;Free memory per GPU:&#34;</span>
</span></span><span class="line"><span class="cl">nvidia-smi --query-gpu<span class="o">=</span>memory.free --format<span class="o">=</span>csv,noheader,nounits <span class="p">|</span> awk <span class="s1">&#39;&#34;&#39;</span><span class="s2">&#34;&#39;{print </span><span class="nv">$1</span><span class="s2">/1024 &#34;</span> GB<span class="s2">&#34;}&#39;&#34;</span><span class="s1">&#39;&#34;&#39;</span> <span class="p">|</span> column -t
</span></span></code></pre></div><p>当我运行脚本时，可以看到如下的输出：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">Every 1.0s:                                 cxhpc: Thu Jan <span class="m">23</span> 19:22:23 <span class="m">2025</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">User_A      <span class="m">60806</span>  59.38  GB  2.47  Cards
</span></span><span class="line"><span class="cl">Me          <span class="m">49092</span>  47.94  GB  2.00  Cards  &lt;<span class="o">==</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">==============================================</span>
</span></span><span class="line"><span class="cl">Memory usage by me:
</span></span><span class="line"><span class="cl">Total memory usage: <span class="m">39969</span> MB <span class="o">(</span><span class="m">39</span> GB<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">==============================================</span>
</span></span><span class="line"><span class="cl">Free memory per GPU:
</span></span><span class="line"><span class="cl">7.61328  GB
</span></span><span class="line"><span class="cl">21.5918  GB
</span></span><span class="line"><span class="cl">3.69727  GB
</span></span><span class="line"><span class="cl">3.69727  GB
</span></span><span class="line"><span class="cl">9.93164  GB
</span></span><span class="line"><span class="cl">21.7715  GB
</span></span><span class="line"><span class="cl">10.1504  GB
</span></span><span class="line"><span class="cl">2.45312  GB
</span></span></code></pre></div><p>为了直观显示显存占用的大小，我直接将显存换算为 RTX 4090 显存 24GB 的倍数，即大约占用了多少张显卡。</p>
<h3 id="gpu-资源分配">GPU 资源分配<a href="#gpu-资源分配" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>使用 pytorch 框架时，可以直接指定使用哪个 GPU，例如：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></div><p>有一些场景下，需要保证所有的张量都在同一个 GPU 上，这时可以使用 Tensor.to() 方法，将某个张量转移到指定的 GPU 上。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda:1&#34;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></div><p>然而，我的实验场景下，最常见的情况是同一个模型要同时在多个数据集上训练。如果在代码中指定某个特定的 GPU，那么无论并行的任务数量多少，所有的任务都会被分配到同一个 GPU 上，大大限制了任务的并行度。</p>
<p>我为此编写了一个简单的 python 函数，这个函数可以根据当前 GPU 的使用情况，自动选择一个较为空闲的 GPU。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">subprocess</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_gpu_memory</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="s1">&#39;nvidia-smi&#39;</span><span class="p">,</span> <span class="s1">&#39;--query-gpu=memory.total,memory.used&#39;</span><span class="p">,</span> <span class="s1">&#39;--format=csv,nounits,noheader&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">stdout</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">returncode</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;nvidia-smi error: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">stderr</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;, &#39;</span><span class="p">)))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_lowest_memory_gpu</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">gpu_memory</span> <span class="o">=</span> <span class="n">get_gpu_memory</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">free_memory</span> <span class="o">=</span> <span class="p">[(</span><span class="n">total</span> <span class="o">-</span> <span class="n">used</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">total</span><span class="p">,</span> <span class="n">used</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gpu_memory</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">best_gpu</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">free_memory</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">_</span> <span class="o">&lt;</span> <span class="mi">6000</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&#34;No free GPU available&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Best GPU: </span><span class="si">{</span><span class="n">best_gpu</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">best_gpu</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">opti_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cuda:</span><span class="si">{</span><span class="n">get_lowest_memory_gpu</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>这个函数会返回一个较为空闲的 GPU，并直接生成一个名为 opti_device 的 torch.device 对象，可以直接用于 pytorch 的代码中。</p>
<p>指定 GPU 为 opti_device 后，pytorch 会自动选择一个较为空闲的 GPU 进行计算。</p>
<p>使用这个 GPU 代码片段，shell 脚本并行任务需要做出一些修改。在提交每个任务需要调用 <code>sleep</code> 命令等待一段时间，使任务有足够的时间把需要的 GPU 显存分配完成。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="cp">#!/bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="nv">max_jobs</span><span class="o">=</span><span class="m">5</span>
</span></span><span class="line"><span class="cl"><span class="nv">current_jobs</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> i in <span class="o">{</span>1..10<span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="k">do</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> <span class="nv">$current_jobs</span> -ge <span class="nv">$max_jobs</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nb">wait</span> -n
</span></span><span class="line"><span class="cl">        <span class="nv">current_jobs</span><span class="o">=</span><span class="k">$((</span>current_jobs-1<span class="k">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">    nohup python main.py --dataset <span class="nv">$i</span> 2&gt;<span class="p">&amp;</span><span class="m">1</span> &gt; output_<span class="nv">$i</span>.log <span class="p">&amp;</span>
</span></span><span class="line"><span class="cl">    <span class="nv">current_jobs</span><span class="o">=</span><span class="k">$((</span>current_jobs+1<span class="k">))</span>
</span></span><span class="line"><span class="cl">    sleep <span class="m">30</span>
</span></span><span class="line"><span class="cl"><span class="k">done</span>
</span></span></code></pre></div><p>如果不使用 sleep 命令，那么最开始的 max_jobs 个任务会在短时间内同时被提交，这时它们获取的最优 GPU 极有可能是同一个，导致单张卡的显存不足。</p>

      </div></div>

  
    
<div class="pagination">
  <div class="pagination__title">
    <span class="pagination__title-h">Read other posts</span>
    <hr />
  </div>
  <div class="pagination__buttons">
    
      <a href="https://Southern-wood.github.io/posts/2025-02-04_aerospaceraycast/" class="button inline prev">
        &lt; [<span class="button__text">Raycast &amp; Aerospace: 打造最具生产力的 Mac 桌面环境</span>]
      </a>
    
    
      ::
    
    
      <a href="https://Southern-wood.github.io/posts/2024-04-30_run-ubuntu-on-mac-with-docker/pic/" class="button inline next">
         [<span class="button__text">如何使用 docker 在 Mac 上运行并配置 Ubuntu</span>] &gt;
      </a>
    
  </div>
</div>


  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>



<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>

<script>

(function(){
  function init(){
    if(typeof renderMathInElement !== 'function') return;
    try{
      renderMathInElement(document.body, {
        
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false}
        ],
        
        ignoredTags: ['script','noscript','style','textarea','pre','code']
      });
    }catch(e){
      console && console.error && console.error('KaTeX render error', e);
    }
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', init);
  } else {
    init();
  }
})();
</script>



  
</div>

</body>
</html>
